# -----------------------------------------------------------------------------
# Copyright (c) 2019, Battelle National Biodefense Institute.
#
# This file is part of MicroHapDB (http://github.com/bioforensics/microhapdb)
# and is licensed under the BSD license: see LICENSE.txt.
# -----------------------------------------------------------------------------


import builtins
from glob import glob
from gzip import open as gzopen
import os
import pandas
from pearhash.pearhash import PearsonHasher
from pyfaidx import Fasta as FastaIdx
import sys


def smartopen(filename, mode):
    """Smart file handler

    Determines whether to create a compressed or un-compressed file handle
    based on filename extension.
    """
    if mode not in ('r', 'w'):
        raise ValueError('invalid mode "{}"'.format(mode))
    if filename in ['-', None]:
        filehandle = sys.stdin if mode == 'r' else sys.stdout
        return filehandle
    openfunc = builtins.open
    if filename.endswith('.gz'):
        openfunc = gzopen
        mode += 't'
    return openfunc(filename, mode)


def load_source_data(table, source):
    src = source.read().strip()
    data = pandas.read_csv(table, sep='\t', index_col=None)
    data['Source'] = src
    return data


def merge_markers(markerfiles, sourcefiles):
    dfs = list()
    for markerfile, sourcefile in zip(markerfiles, sourcefiles):
        with open(sourcefile, 'r') as fh:
            df = load_source_data(markerfile, fh)
            dfs.append(df)
    data = pandas.concat(dfs, axis=0, ignore_index=True)
    return data


def construct_variant_map(data):
    varlist, markerlist = list(), list()
    for n, row in data.iterrows():
        marker = row.Name
        numoffsets = row.Offsets.count(',') + 1
        numrsids = row.VarRef.count(',') + 1
        if numoffsets != numrsids:
            message = 'marker "{name:s}" has {no:d} variants but only {nv:d} rsids'.format(
                name=marker, no=numoffsets, nv=numrsids
            )
            print('WARNING:', message, file=sys.stderr)
        for variant in row.VarRef.split(','):
            varlist.append(variant)
            markerlist.append(marker)
    outdata = pandas.DataFrame({'Variant': varlist, 'Marker': markerlist})
    return outdata.sort_values(['Variant', 'Marker'])


def populate_idmap(data):
    idmap = {
        'Xref': list(),
        'ID': list(),
    }
    for n, row in data.iterrows():
        if row['Xref'] == '' or row['Xref'] is None or pandas.isnull(row['Xref']):
            continue
        for xref in row['Xref'].split(','):
            idmap['Xref'].append(xref)
            idmap['ID'].append(row['Name'])
    data = pandas.DataFrame(idmap).sort_values('ID')
    return data


def parse_fasta(data):
    """Load sequences in Fasta format.
    This generator function yields a tuple containing a defline and a sequence
    for each record in the Fasta data. Stolen shamelessly from
    http://stackoverflow.com/a/7655072/459780.
    """
    name, seq = None, []
    for line in data:
        line = line.rstrip()
        if line.startswith('>'):
            if name:
                yield (name, ''.join(seq))
            name, seq = line, []
        else:
            seq.append(line)
    if name:  # pragma: no cover
        yield (name, ''.join(seq))


def add_marker_permids(data, refr):
    seqs = FastaIdx(refr)

    permids = list()
    hasher = PearsonHasher(4)
    for n, row in data.iterrows():
        offsets = [int(o) for o in row['Offsets'].split(',')]
        minpos, maxpos = min(offsets), max(offsets) + 1
        localoffsets = [o - offsets[0] for o in offsets]
        markerseq = seqs[row['Chrom']][minpos:maxpos].seq
        markerseq = str(markerseq).upper()
        localoffsets = list(map(str, localoffsets))
        hash_input = ','.join(localoffsets + [markerseq])
        marker_hash = hasher.hash(hash_input.encode('utf-8')).hexdigest()
        permid = 'MHDBM-' + marker_hash
        permids.append(permid)

    data['PermID'] = pandas.Series(permids)


def parse_rsid_coords(vcf):
    rsidcoords = dict()
    for line in vcf:
        if line.startswith('#'):
            continue
        chrnum, posstr, rsid, *values = line.strip().split()
        chrom = chrnum if chrnum.startswith('chr') else 'chr' + chrnum
        pos = int(posstr) - 1
        rsidcoords[rsid] = (chrom, pos)
    return rsidcoords


def resolve_offsets(data, vcf37, vcf38):
    with smartopen(vcf37, 'r') as fh:
        rsidcoords37 = parse_rsid_coords(fh)
    with smartopen(vcf38, 'r') as fh:
        rsidcoords38 = parse_rsid_coords(fh)
    o37data = {
        'Marker': list(),
        'Offsets': list(),
    }
    o38data = {
        'Marker': list(),
        'Offsets': list(),
    }
    for n, row in data.iterrows():
        rsids = row.VarRef.split(',')
        o37data['Marker'].append(row.Name)
        o38data['Marker'].append(row.Name)
        if len(rsids) == row.NumVars:
            offsets37 = [rsidcoords37[rsid][1] for rsid in rsids]
            offsetstr37 = ','.join(map(str, offsets37))
            if not pandas.isna(row.OffsetsHg37):
                assert offsetstr37 == row.OffsetsHg37, (offsetstr37, row)
            o37data['Offsets'].append(offsetstr37)
            offsets38 = [rsidcoords38[rsid][1] for rsid in rsids]
            offsetstr38 = ','.join(map(str, offsets38))
            if not pandas.isna(row.OffsetsHg38):
                assert offsetstr38 == row.OffsetsHg38, (offsetstr38, row)
            o38data['Offsets'].append(offsetstr38)
        else:
            message = f'WARNING: {row.NumVars:.0f} variants defined for marker "{row.Name}"'
            message += f' but only {len(rsids)} rsIDS; depending on explicitly provided offsets'
            print(message, file=sys.stderr)
            assert not pandas.isna(row.OffsetsHg37)
            assert not pandas.isna(row.OffsetsHg38)
            assert row.OffsetsHg37.count(',') + 1 == row.NumVars
            assert row.OffsetsHg38.count(',') + 1 == row.NumVars
            o37data['Offsets'].append(row.OffsetsHg37)
            o38data['Offsets'].append(row.OffsetsHg38)
    o37data['Reference'] = 'GRCh37'
    o38data['Reference'] = 'GRCh38'
    o37 = pandas.DataFrame(o37data)
    o38 = pandas.DataFrame(o38data)
    data = data.join(o38.set_index('Marker'), on='Name')
    return data, o37


def sort_and_clean(data):
    data['ChromSort'] = data.Chrom.apply(lambda c: '0X' if c == 'chrX' else int(c[3:]))
    data['TempPos'] = data.Offsets.apply(lambda os: int(os.split(',')[0]))
    data = data.sort_values(['ChromSort', 'TempPos'])
    # FIXME!!! # data.drop(columns=['Xref', 'VarRef', 'ChromSort', 'TempPos'], inplace=True)
    nr = data.drop_duplicates(subset=('Name', 'Chrom', 'Offsets'))
    assert(len(nr.Name) == len(nr.Name.unique()))  # If markers are defined in more than one place, make sure the definitions are identical.
    return data[['Name', 'PermID', 'Reference', 'Chrom', 'Offsets', 'Ae', 'In', 'Fst', 'Source']]


def add_avgae(data, aefile):
    aes = pandas.read_csv(aefile, sep='\t')
    avgae = {'Marker': list(), 'Ae': list()}
    for marker, mdata in aes.groupby('Marker'):
        assert len(mdata) == 26
        meanae = mdata.Ae.mean()
        avgae['Marker'].append(marker)
        avgae['Ae'].append(meanae)
    return data.join(pandas.DataFrame(avgae).set_index('Marker'), on='Name')


def add_informativeness(data, informfile):
    info = pandas.read_csv(informfile, sep='\t').rename(columns={'Informativeness': 'In'})
    return data.join(info.set_index('Marker'), on='Name')


def add_fst(data, fstfile):
    info = pandas.read_csv(fstfile, sep='\t')
    return data.join(info.set_index('Marker'), on='Name')


SOURCES = [os.path.basename(file) for file in glob('sources/*') if os.path.isdir(file)]


rule tables:
    input:
        expand('{table}.tsv', table=['marker', 'population', 'frequency', 'idmap', 'variantmap', 'sequences', 'indels'])


rule populations:
    input:
        expand('sources/{source}/population.tsv', source=SOURCES),
        expand('sources/{source}/source.txt', source=SOURCES),
    output:
        table='population.tsv',
        idmap='pop-idmap.tsv'
    run:
        numsources = int(len(input) / 2)
        popfiles = input[:numsources]
        sourcefiles = input[numsources:]

        dfs = list()
        for popfile, sourcefile in zip(popfiles, sourcefiles):
            with open(sourcefile, 'r') as fh:
                df = load_source_data(popfile, fh)
                dfs.append(df)
        data = pandas.concat(dfs, axis=0, ignore_index=True)
        assert len(data.ID) == len(data.ID.unique())

        idmap = {
            'Xref': list(),
            'ID': list()
        }
        subdata = data[(data.Xref != '') & (data.Xref.notnull())]
        for n, row in subdata.iterrows():
            for xref in row['Xref'].split(','):
                idmap['Xref'].append(xref)
                idmap['ID'].append(row['ID'])
        idmapdf = pandas.DataFrame(idmap)
        idmapdf.sort_values('ID').to_csv(output.idmap, sep='\t', index=False)

        data.drop(columns=['Xref'], inplace=True)
        data.sort_values(['Name', 'ID']).to_csv(output.table, sep='\t', index=False)


rule frequencies:
    input: expand('sources/{source}/frequency.tsv', source=SOURCES)
    output: 'frequency.tsv'
    run:
        dfs = list()
        for freqfile in input:
            df = pandas.read_csv(freqfile, sep='\t', index_col=None, float_precision='round_trip')
            dfs.append(df)
        data = pandas.concat(dfs, axis=0, ignore_index=True)
        data.sort_values(['Marker', 'Population']).to_csv(output[0], sep='\t', index=False)


rule dbsnp_subset:
    input:
        expand('sources/{source}/marker.tsv', source=SOURCES),
        dbsnp37=config['dbsnp37'],
        dbsnp38=config['dbsnp38'],
        rsidx37=config['dbsnp37'].replace('.vcf.gz', '.rsidx'),
        rsidx38=config['dbsnp38'].replace('.vcf.gz', '.rsidx'),
    output:
        sub37='dbsnp-subset-GRCh37.vcf.gz',
        sub38='dbsnp-subset-GRCh38.vcf.gz',
    run:
        rsids = list()
        for infile in input[:-4]:
            markers = pandas.read_csv(infile, sep='\t')
            for n, row in markers.iterrows():
                rsids.extend(row.VarRef.split(','))
        rsids = ' '.join(rsids)
        command = f'rsidx search --header --out {output.sub37} {input.dbsnp37} {input.rsidx37} {rsids}'
        shell(command)
        command = f'rsidx search --header --out {output.sub38} {input.dbsnp38} {input.rsidx38} {rsids}'
        shell(command)


rule markers:
    input:
        config['refr'],
        'dbsnp-subset-GRCh37.vcf.gz',
        'dbsnp-subset-GRCh38.vcf.gz',
        'sources/1kgp/marker-aes.tsv',
        'sources/1kgp/marker-informativeness.tsv',
        'sources/1kgp/marker-fst.tsv',
        expand('sources/{source}/marker.tsv', source=SOURCES),
        expand('sources/{source}/source.txt', source=SOURCES),
    output:
        table='marker.tsv',
        varmap='variantmap.tsv',
        idmap='marker-idmap.tsv',
        offsets37='marker-offsets-GRCh37.tsv',
    run:
        dbsnpsub37 = input[1]
        dbsnpsub38 = input[2]
        aefile = input[3]
        informfile = input[4]
        fstfile = input[5]
        input = input[6:]
        numsources = int(len(input) / 2)
        markerfiles = input[:numsources]
        sourcefiles = input[numsources:]

        data = merge_markers(markerfiles, sourcefiles)
        data, offsets37 = resolve_offsets(data, dbsnpsub37, dbsnpsub38)
        add_marker_permids(data, config['refr'])
        data = add_avgae(data, aefile)
        data = add_informativeness(data, informfile)
        data = add_fst(data, fstfile)
        idmap = populate_idmap(data)
        varmap = construct_variant_map(data)
        data = sort_and_clean(data)
        idmap.to_csv(output.idmap, sep='\t', index=False)
        varmap.to_csv(output.varmap, sep='\t', index=False)
        data.to_csv(output.table, sep='\t', index=False, float_format='%.4f')
        offsets37.to_csv(output.offsets37, sep='\t', index=False)


rule idmap:
    input:
        'pop-idmap.tsv',
        'marker-idmap.tsv'
    output: 'idmap.tsv'
    run:
        dfs = list()
        for mapfile in input:
            df = pandas.read_csv(mapfile, sep='\t', index_col=None)
            dfs.append(df)
        data = pandas.concat(dfs, axis=0, ignore_index=True)
        data.sort_values(['ID', 'Xref']).to_csv(output[0], sep='\t', index=False)


rule sequences:
    input:
        markers='marker.tsv',
        refr=config['refr'],
    output:
        seqs='sequences.tsv'
    run:
        seqs = FastaIdx(input.refr)
        markers = pandas.read_csv(input.markers, sep='\t')
        with open(output.seqs, 'w') as fh:
            print('Marker', 'Chrom', 'LeftFlank', 'RightFlank', 'Sequence', sep='\t', file=fh)
            for n, row in markers.iterrows():
                offsets = [int(o) for o in row['Offsets'].split(',')]
                minpos, maxpos = min(offsets) - 500, max(offsets) + 501
                flankingseq = seqs[row['Chrom']][minpos:maxpos].seq
                flankingseq = str(flankingseq).upper()
                print(row['Name'], row['Chrom'], minpos, maxpos, flankingseq, sep='\t', file=fh)


rule indels:
    input: glob('sources/*/indels.tsv')
    output: 'indels.tsv'
    run:
        dfs = list()
        for freqfile in input:
            df = pandas.read_csv(freqfile, sep='\t', index_col=None)
            dfs.append(df)
        data = pandas.concat(dfs, axis=0, ignore_index=True)
        data.sort_values(['Marker', 'VariantIndex']).to_csv(output[0], sep='\t', index=False)
