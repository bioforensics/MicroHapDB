# -----------------------------------------------------------------------------
# Copyright (c) 2019, Battelle National Biodefense Institute.
#
# This file is part of MicroHapDB (http://github.com/bioforensics/microhapdb)
# and is licensed under the BSD license: see LICENSE.txt.
# -----------------------------------------------------------------------------

from glob import glob
import os
import pandas
from pearhash.pearhash import PearsonHasher
from pyfaidx import Fasta as FastaIdx
import sys


def load_source_data(table, source):
    src = source.read().strip()
    data = pandas.read_csv(table, sep='\t', index_col=None)
    data['Source'] = src
    return data


def merge_markers(markerfiles, sourcefiles):
    dfs = list()
    for markerfile, sourcefile in zip(markerfiles, sourcefiles):
        with open(sourcefile, 'r') as fh:
            df = load_source_data(markerfile, fh)
            dfs.append(df)
    data = pandas.concat(dfs, axis=0, ignore_index=True)
    return data


def construct_variant_map(data):
    varlist, markerlist = list(), list()
    for n, row in data.iterrows():
        marker = row.Name
        numoffsets = row.Offsets.count(',') + 1
        numrsids = row.VarRef.count(',') + 1
        if numoffsets != numrsids:
            message = 'marker "{name:s}" has {no:d} variants but only {nv:d} rsids'.format(
                name=marker, no=numoffsets, nv=numrsids
            )
            print('WARNING:', message, file=sys.stderr)
        for variant in row.VarRef.split(','):
            varlist.append(variant)
            markerlist.append(marker)
    outdata = pandas.DataFrame({'Variant': varlist, 'Marker': markerlist})
    return outdata.sort_values('Variant')


def populate_idmap(data):
    idmap = {
        'Xref': list(),
        'ID': list(),
    }
    for n, row in data.iterrows():
        if row['Xref'] == '' or row['Xref'] is None or pandas.isnull(row['Xref']):
            continue
        for xref in row['Xref'].split(','):
            idmap['Xref'].append(xref)
            idmap['ID'].append(row['Name'])
    data = pandas.DataFrame(idmap).sort_values('ID')
    return data


def parse_fasta(data):
    """Load sequences in Fasta format.
    This generator function yields a tuple containing a defline and a sequence
    for each record in the Fasta data. Stolen shamelessly from
    http://stackoverflow.com/a/7655072/459780.
    """
    name, seq = None, []
    for line in data:
        line = line.rstrip()
        if line.startswith('>'):
            if name:
                yield (name, ''.join(seq))
            name, seq = line, []
        else:
            seq.append(line)
    if name:  # pragma: no cover
        yield (name, ''.join(seq))


def add_marker_permids(data, refr):
    seqs = FastaIdx(refr)

    permids = list()
    hasher = PearsonHasher(4)
    for n, row in data.iterrows():
        offsets = [int(o) for o in row['Offsets'].split(',')]
        minpos, maxpos = min(offsets), max(offsets) + 1
        localoffsets = [o - offsets[0] for o in offsets]
        markerseq = seqs[row['Chrom']][minpos:maxpos].seq
        markerseq = str(markerseq).upper()
        localoffsets = list(map(str, localoffsets))
        hash_input = ','.join(localoffsets + [markerseq])
        marker_hash = hasher.hash(hash_input.encode('utf-8')).hexdigest()
        permid = 'MHDBM-' + marker_hash
        permids.append(permid)

    data['PermID'] = pandas.Series(permids)


def compute_marker_ae(data, freqs):
    marker_nvars = dict()
    for n, row in data.iterrows():
        nvars = row.Offsets.count(',') + 1
        marker = row.Name
        marker_nvars[marker] = nvars

    aedata = {'Marker': list(), 'Population': list(), 'Ae': list()}
    pops1kgp = set([
        'CHB', 'JPT', 'CHS', 'CDX', 'KHV', 'CEU', 'TSI', 'FIN', 'GBR', 'IBS', 'YRI', 'LWK', 'GWD',
        'MSL', 'ESN', 'ASW', 'ACB', 'MXL', 'PUR', 'CLM', 'PEL', 'GIH', 'PJL', 'BEB', 'STU', 'ITU'
    ])
    for marker, mdata in freqs[freqs.Population.isin(pops1kgp)].groupby('Marker'):
        for population, pdata in mdata.groupby('Population'):
            for haplotype in pdata.Allele.unique():
                nvars = haplotype.count(',') + 1
                if nvars != marker_nvars[marker]:
                    message = f'allele count disagreement for {marker}:'
                    message += f' {nvars} vs {marker_nvars[marker]} ({haplotype})'
                    raise ValueError(message)
            ae = 1.0 / sum([f ** 2 for f in pdata.Frequency])
            aedata['Marker'].append(marker)
            aedata['Population'].append(population)
            aedata['Ae'].append(ae)
    aes = pandas.DataFrame(aedata).sort_values(['Marker', 'Population'])

    avgae = {'Marker': list(), 'AvgAe': list()}
    for marker, mdata in aes.groupby('Marker'):
        assert len(mdata) == len(pops1kgp)
        meanae = mdata.Ae.mean()
        avgae['Marker'].append(marker)
        avgae['AvgAe'].append(meanae)
    return data.join(pandas.DataFrame(avgae).set_index('Marker'), on='Name'), aes


def sort_and_clean(data):
    data['ChromSort'] = data.Chrom.apply(lambda c: '0X' if c == 'chrX' else int(c[3:]))
    data['TempPos'] = data.Offsets.apply(lambda os: int(os.split(',')[0]))
    data = data.sort_values(['ChromSort', 'TempPos'])
    data.drop(columns=['Xref', 'VarRef', 'ChromSort', 'TempPos'], inplace=True)
    nr = data.drop_duplicates(subset=('Name', 'Chrom', 'Offsets'))
    assert(len(nr.Name) == len(nr.Name.unique()))  # If markers are defined in more than one place, make sure the definitions are identical.
    return data[['Name', 'PermID', 'Reference', 'Chrom', 'Offsets', 'AvgAe', 'Source']]


def add_informativeness(data, informfile):
    info = pandas.read_csv(informfile, sep='\t')
    data['In'] = None
    for n, row in info.iterrows():
        data.loc[data.Name == row.Marker, 'In'] = row.Informativeness
    return data[['Name', 'PermID', 'Reference', 'Chrom', 'Offsets', 'AvgAe', 'In', 'Source']]


SOURCES = [os.path.basename(file) for file in glob('sources/*') if os.path.isdir(file)]


rule tables:
    input:
        expand('{table}.tsv', table=['marker', 'population', 'frequency', 'idmap', 'variantmap', 'sequences', 'indels'])


rule populations:
    input:
        expand('sources/{source}/population.tsv', source=SOURCES),
        expand('sources/{source}/source.txt', source=SOURCES),
    output:
        table='population.tsv',
        idmap='pop-idmap.tsv'
    run:
        numsources = int(len(input) / 2)
        popfiles = input[:numsources]
        sourcefiles = input[numsources:]

        dfs = list()
        for popfile, sourcefile in zip(popfiles, sourcefiles):
            with open(sourcefile, 'r') as fh:
                df = load_source_data(popfile, fh)
                dfs.append(df)
        data = pandas.concat(dfs, axis=0, ignore_index=True)
        assert len(data.ID) == len(data.ID.unique())

        idmap = {
            'Xref': list(),
            'ID': list()
        }
        subdata = data[(data.Xref != '') & (data.Xref.notnull())]
        for n, row in subdata.iterrows():
            for xref in row['Xref'].split(','):
                idmap['Xref'].append(xref)
                idmap['ID'].append(row['ID'])
        idmapdf = pandas.DataFrame(idmap)
        idmapdf.sort_values('ID').to_csv(output.idmap, sep='\t', index=False)

        data.drop(columns=['Xref'], inplace=True)
        data.sort_values(['Name', 'ID']).to_csv(output.table, sep='\t', index=False)


rule frequencies:
    input: expand('sources/{source}/frequency.tsv', source=SOURCES)
    output: 'frequency.tsv'
    run:
        dfs = list()
        for freqfile in input:
            df = pandas.read_csv(freqfile, sep='\t', index_col=None, float_precision='round_trip')
            dfs.append(df)
        data = pandas.concat(dfs, axis=0, ignore_index=True)
        data.sort_values(['Marker', 'Population']).to_csv(output[0], sep='\t', index=False)


rule markers:
    input:
        config['refr'],
        'frequency.tsv',
        'sources/1kgp/marker-informativeness.tsv',
        expand('sources/{source}/marker.tsv', source=SOURCES),
        expand('sources/{source}/source.txt', source=SOURCES),
    output:
        table='marker.tsv',
        varmap='variantmap.tsv',
        aes='aes.tsv',
        idmap='marker-idmap.tsv'
    run:
        freqfile = input[1]
        informfile = input[2]
        input = input[3:]
        numsources = int(len(input) / 2)
        markerfiles = input[:numsources]
        sourcefiles = input[numsources:]

        data = merge_markers(markerfiles, sourcefiles)
        varmap = construct_variant_map(data)
        varmap.to_csv(output.varmap, sep='\t', index=False)
        add_marker_permids(data, config['refr'])
        freqs = pandas.read_csv(freqfile, sep='\t')
        data, aes = compute_marker_ae(data, freqs)
        aes.to_csv(output.aes, sep='\t', index=False, float_format='%.4f')

        idmap = populate_idmap(data)
        idmap.to_csv(output.idmap, sep='\t', index=False)

        data = sort_and_clean(data)
        data = add_informativeness(data, informfile)
        data.to_csv(output.table, sep='\t', index=False, float_format='%.4f')


rule idmap:
    input:
        'pop-idmap.tsv',
        'marker-idmap.tsv'
    output: 'idmap.tsv'
    run:
        dfs = list()
        for mapfile in input:
            df = pandas.read_csv(mapfile, sep='\t', index_col=None)
            dfs.append(df)
        data = pandas.concat(dfs, axis=0, ignore_index=True)
        data.sort_values(['ID', 'Xref']).to_csv(output[0], sep='\t', index=False)


rule sequences:
    input:
        markers='marker.tsv',
        refr=config['refr'],
    output:
        seqs='sequences.tsv'
    run:
        seqs = FastaIdx(input.refr)
        markers = pandas.read_csv(input.markers, sep='\t')
        with open(output.seqs, 'w') as fh:
            print('Marker', 'Chrom', 'LeftFlank', 'RightFlank', 'Sequence', sep='\t', file=fh)
            for n, row in markers.iterrows():
                offsets = [int(o) for o in row['Offsets'].split(',')]
                minpos, maxpos = min(offsets) - 500, max(offsets) + 501
                flankingseq = seqs[row['Chrom']][minpos:maxpos].seq
                flankingseq = str(flankingseq).upper()
                print(row['Name'], row['Chrom'], minpos, maxpos, flankingseq, sep='\t', file=fh)


rule indels:
    input: glob('sources/*/indels.tsv')
    output: 'indels.tsv'
    run:
        dfs = list()
        for freqfile in input:
            df = pandas.read_csv(freqfile, sep='\t', index_col=None)
            dfs.append(df)
        data = pandas.concat(dfs, axis=0, ignore_index=True)
        data.sort_values('Marker').to_csv(output[0], sep='\t', index=False)
