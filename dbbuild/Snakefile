# -----------------------------------------------------------------------------
# Copyright (c) 2019, Battelle National Biodefense Institute.
#
# This file is part of MicroHapDB (http://github.com/bioforensics/microhapdb)
# and is licensed under the BSD license: see LICENSE.txt.
# -----------------------------------------------------------------------------


from glob import glob
import os

import pandas


def load_source_data(table, source):
    src = source.read().strip()
    data = pandas.read_csv(table, sep='\t', index_col=None)
    data['Source'] = src
    return data


def write_variant_map(data, output):
    print('Variant', 'Marker', sep='\t', file=output)
    for n, row in data.iterrows():
        marker = row['Name']
        for variant in row['VarRef'].split(','):
            print(variant, marker, sep='\t', file=output)


def populate_idmap(data):
    idmap = {
        'Xref': list(),
        'ID': list(),
    }
    for n, row in data.iterrows():
        if row['Xref'] == '' or row['Xref'] is None or pandas.isnull(row['Xref']):
            continue
        for xref in row['Xref'].split(','):
            idmap['Xref'].append(xref)
            idmap['ID'].append(row['Name'])
    data = pandas.DataFrame(idmap).sort_values('ID')
    return data


SOURCES = [os.path.basename(file) for file in glob('sources/*') if os.path.isdir(file)]


rule tables:
    input:
        expand('{table}.tsv', table=['marker', 'population', 'frequency', 'idmap', 'variantmap'])


rule populations:
    input:
        expand('sources/{source}/population.tsv', source=SOURCES),
        expand('sources/{source}/source.txt', source=SOURCES),
    output:
        table='population.tsv'
    run:
        numsources = int(len(input) / 2)
        popfiles = input[:numsources]
        sourcefiles = input[numsources:]

        dfs = list()
        for popfile, sourcefile in zip(popfiles, sourcefiles):
            with open(sourcefile, 'r') as fh:
                df = load_source_data(popfile, fh)
                dfs.append(df)
        data = pandas.concat(dfs, axis=0, ignore_index=True)
        assert len(data.ID) == len(data.ID.unique())
        data.sort_values('Name').to_csv(output.table, sep='\t', index=False)


rule frequencies:
    input: expand('sources/{source}/frequency.tsv', source=SOURCES)
    output: 'frequency.tsv'
    run:
        dfs = list()
        for freqfile in input:
            df = pandas.read_csv(freqfile, sep='\t', index_col=None)
            dfs.append(df)
        data = pandas.concat(dfs, axis=0, ignore_index=True)
        data.sort_values(['Marker', 'Population']).to_csv(output[0], sep='\t', index=False)


rule markers:
    input:
        expand('sources/{source}/marker.tsv', source=SOURCES),
        expand('sources/{source}/source.txt', source=SOURCES),
    output:
        table='marker.tsv',
        varmap='variantmap.tsv',
        idmap='idmap.tsv'
    run:
        numsources = int(len(input) / 2)
        markerfiles = input[:numsources]
        sourcefiles = input[numsources:]

        dfs = list()
        for markerfile, sourcefile in zip(markerfiles, sourcefiles):
            with open(sourcefile, 'r') as fh:
                df = load_source_data(markerfile, fh)
                dfs.append(df)

        data = pandas.concat(dfs, axis=0, ignore_index=True)
        with open(output.varmap, 'w') as fh:
            write_variant_map(data, fh)

        # FIXME Add Pearson hash and AvgAe

        idmap = populate_idmap(data)
        idmap.to_csv(output.idmap, sep='\t', index=False)

        data.drop(columns=['Xref', 'VarRef'], inplace=True)
        data.to_csv(output.table, sep='\t', index=False)
